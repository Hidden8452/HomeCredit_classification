{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YSwoVkXy7ttB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "os3cB5_hB_Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "sxoT4tPgCA45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data import"
      ],
      "metadata": {
        "id": "VSs8stCmCGyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XrIDEwjTsVxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.expand_frame_repr', False)"
      ],
      "metadata": {
        "id": "oKVUsWRKCGD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explore = pd.read_csv(\"HomeCredit_columns_description.csv\", encoding='latin1')\n",
        "apl_train = pd.read_csv(\"application_train.csv\")\n",
        "apl_test = pd.read_csv(\"application_test.csv\")\n",
        "pos = pd.read_csv(\"POS_CASH_balance.csv\")\n",
        "bur = pd.read_csv(\"bureau.csv\")\n",
        "bur_bal = pd.read_csv(\"bureau_balance.csv\")\n",
        "credit = pd.read_csv(\"credit_card_balance.csv\")\n",
        "prev = pd.read_csv(\"previous_application.csv\")\n",
        "inst = pd.read_csv(\"installments_payments.csv\")"
      ],
      "metadata": {
        "id": "ypMi2qKGCMnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## When merged, import train table"
      ],
      "metadata": {
        "id": "KVNd90iEsaSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('/content/drive/MyDrive/balanced_train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/destination_folder')  # Adjust the path and extraction folder"
      ],
      "metadata": {
        "id": "_qzGChcKsOcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions"
      ],
      "metadata": {
        "id": "rDE4QC3eB6sd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN2xINjEOCDH"
      },
      "outputs": [],
      "source": [
        "#  Performs statistical analysis on all dataframes\n",
        "\n",
        "def perform_statistical_analysis(df):\n",
        "    # Initialize a list to collect summary statistics\n",
        "    summary_rows = []\n",
        "\n",
        "    # Loop through each column in the DataFrame\n",
        "    for column in df.columns:\n",
        "        # Check if the column is numerical\n",
        "        if df[column].dtype in [int, float]:\n",
        "            # Compute descriptive statistics\n",
        "            stats = df[column].describe()\n",
        "            # Convert Series to a dictionary and add the column name\n",
        "            stats_dict = stats.to_dict()\n",
        "            stats_dict['Column'] = column\n",
        "            # Calculate and add the number of missing values\n",
        "            stats_dict['missing_count'] = df[column].isnull().sum()\n",
        "            # Calculate and add the percentage of missing values\n",
        "            stats_dict['% of missing'] = (stats_dict['missing_count'] / len(df)) * 100\n",
        "            # Add the number of unique values\n",
        "            stats_dict['unique_count'] = df[column].nunique()\n",
        "\n",
        "            # Add the dictionary to the list of rows\n",
        "            summary_rows.append(stats_dict)\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame\n",
        "    summary_df = pd.DataFrame(summary_rows)\n",
        "\n",
        "    # Reorder columns to have 'Column' first\n",
        "    columns_order = ['Column', 'count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max', 'missing_count', '% of missing', 'unique_count']\n",
        "    summary_df = summary_df[columns_order]\n",
        "\n",
        "    # Transpose the DataFrame for better readability\n",
        "    summary_df = summary_df.set_index('Column').transpose()\n",
        "\n",
        "    # Display the summary DataFrame\n",
        "    display(summary_df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_cols(df, threshold=40):\n",
        "\n",
        "    # Calculate the percentage of missing values for each column\n",
        "    missing_count_pct = df.isnull().sum() / len(df) * 100\n",
        "\n",
        "    # Drop columns where the percentage of missing values is greater than the threshold\n",
        "    cols_to_drop = missing_count_pct[missing_count_pct > threshold].index\n",
        "    df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "UAFNAR-SLOeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_columns(df):\n",
        "    # Transpose the DataFrame to compare columns as rows\n",
        "    transposed_df = df.T\n",
        "\n",
        "    # Drop duplicate rows in the transposed DataFrame, then transpose back\n",
        "    deduplicated_df = transposed_df.drop_duplicates().T\n",
        "\n",
        "    return deduplicated_df"
      ],
      "metadata": {
        "id": "206JQvOmtegl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cols_to_drop(df, threshold_1 = 25, threshold_2 = 5):\n",
        "\n",
        "    cols_to_drop = []\n",
        "\n",
        "    # Calculate the percentage of missing values for each column\n",
        "    missing_count_pct = df.isnull().sum() / len(df) * 100\n",
        "\n",
        "    # Loop through each column in the DataFrame\n",
        "    for col in df.columns:\n",
        "        # Check if the missing value percentage is above the threshold\n",
        "        if missing_count_pct[col] > threshold_1:\n",
        "            cols_to_drop.append(col)\n",
        "\n",
        "        # Check if the column is of object type and has more unique values than the threshold\n",
        "        if df[col].dtype == 'object' and df[col].nunique() > threshold_2:\n",
        "            cols_to_drop.append(col)\n",
        "\n",
        "    return cols_to_drop"
      ],
      "metadata": {
        "id": "eeTqh6czOgCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correlation_with_value(df, target_column, value):\n",
        "    # Check if the target column exists in the DataFrame\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"Column '{target_column}' does not exist in the DataFrame.\")\n",
        "        return\n",
        "\n",
        "    # Check if the target column's data type is numeric\n",
        "    if np.issubdtype(df[target_column].dtype, np.number):\n",
        "        # Create a new DataFrame with only numeric columns\n",
        "        numerical_df = df.select_dtypes(include=[np.number])\n",
        "\n",
        "        # Compute the correlation matrix\n",
        "        correlation_matrix = numerical_df.corr()\n",
        "\n",
        "        # Ensure the target column exists in the correlation matrix\n",
        "        if target_column in correlation_matrix.columns:\n",
        "            # Compute correlation of each numeric column with the target column\n",
        "            correlations = {}\n",
        "            for col in numerical_df.columns:\n",
        "                if col != target_column:\n",
        "                    correlations[col] = df[[col, target_column]].corr().iloc[0, 1]\n",
        "\n",
        "            # Convert correlations dictionary to a DataFrame for better readability\n",
        "            correlations_df = pd.DataFrame(list(correlations.items()), columns=['Column', 'Correlation'])\n",
        "\n",
        "            # Sort by correlation values\n",
        "            correlations_df = correlations_df.sort_values(by='Correlation', ascending=False)\n",
        "            return correlations_df\n",
        "\n",
        "            # Print the correlation values\n",
        "            print(f\"Correlation of numeric columns with '{target_column}':\")\n",
        "\n",
        "            print(correlations_df)\n",
        "        else:\n",
        "            print(f\"Column '{target_column}' is not present in the correlation matrix.\")\n",
        "    else:\n",
        "        print(f\"Column '{target_column}' is not numeric. Correlation analysis is not applicable.\")"
      ],
      "metadata": {
        "id": "WsWrQisy7zzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_data(data, groupby_column, label=None):\n",
        "    ### SEPARATE FEATURES\n",
        "\n",
        "    # Display info\n",
        "    print(\"- Preparing the dataset...\")\n",
        "\n",
        "    # Find factors (categorical columns)\n",
        "    data_factors = [f for f in data.columns if data[f].dtype == \"object\"]\n",
        "\n",
        "    # Partition subsets\n",
        "    num_data = data.drop(columns=data_factors)\n",
        "    fac_data = data[[groupby_column] + data_factors]\n",
        "\n",
        "    # Display info\n",
        "    num_facs = fac_data.shape[1] - 1  # subtract 1 for the groupby column\n",
        "    num_nums = num_data.shape[1]\n",
        "    print(f\"- Extracted {num_facs} factors and {num_nums} numerics...\")\n",
        "\n",
        "    ##### AGGREGATION\n",
        "\n",
        "    # Aggregate numeric data\n",
        "    if num_nums > 0:\n",
        "        print(\"- Aggregating numeric features...\")\n",
        "        num_data = num_data.groupby(data[groupby_column]).agg([\"mean\", \"std\", \"min\", \"max\"])\n",
        "        num_data.columns = [\"_\".join(col).strip() for col in num_data.columns.values]\n",
        "        num_data = num_data.sort_index()\n",
        "\n",
        "    # Aggregate categorical data\n",
        "    if num_facs > 0:\n",
        "        print(\"- Aggregating factor features...\")\n",
        "        fac_data = fac_data.groupby(groupby_column).agg([\n",
        "            (\"mode\", lambda x: x.mode()[0] if not x.mode().empty else None),\n",
        "            (\"unique\", lambda x: x.nunique())\n",
        "        ])\n",
        "        fac_data.columns = [\"_\".join(col).strip() for col in fac_data.columns.values]\n",
        "        fac_data = fac_data.sort_index()\n",
        "\n",
        "    ##### MERGER\n",
        "\n",
        "    # Merge numeric and categorical data\n",
        "    if num_nums > 0 and num_facs > 0:\n",
        "        agg_data = pd.concat([num_data, fac_data], axis=1)\n",
        "    elif num_nums > 0:\n",
        "        agg_data = num_data\n",
        "    elif num_facs > 0:\n",
        "        agg_data = fac_data\n",
        "    else:\n",
        "        agg_data = pd.DataFrame()\n",
        "\n",
        "    ##### LAST STEPS\n",
        "\n",
        "    # Update labels if provided\n",
        "    if label is not None:\n",
        "        agg_data.columns = [f\"{label}_{col}\" for col in agg_data.columns]\n",
        "\n",
        "    # Display info\n",
        "    print(\"- Final dimensions:\", agg_data.shape)\n",
        "\n",
        "    # Return the aggregated dataset\n",
        "    return agg_data"
      ],
      "metadata": {
        "id": "IubQYR7S4OCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_log_transform_columns(df, skew_threshold=1.0):\n",
        "\n",
        "    columns_to_transform = []\n",
        "\n",
        "    for col in df.select_dtypes(include=[np.number]).columns:\n",
        "        # Calculate the skewness\n",
        "        skewness = df[col].skew()\n",
        "\n",
        "        # Check if skewness is above the threshold\n",
        "        if abs(skewness) > skew_threshold:\n",
        "            columns_to_transform.append(col)\n",
        "\n",
        "    return columns_to_transform"
      ],
      "metadata": {
        "id": "7qT77MUhBk6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_transform_columns(df, columns_to_transform):\n",
        "\n",
        "    df_transformed = df.copy()\n",
        "    for col in columns_to_transform:\n",
        "        if col in df_transformed.columns:\n",
        "\n",
        "            df_transformed[col] = df_transformed[col].apply(lambda x: np.log(x + 1) if x > 0 else 0)\n",
        "\n",
        "    return df_transformed"
      ],
      "metadata": {
        "id": "aEf5HZT7c_ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def high_correlation_columns(df, threshold):\n",
        "    # Calculate the correlation matrix\n",
        "    corr_matrix = df.corr().abs()  # Use absolute values to consider the magnitude\n",
        "\n",
        "    # Create a mask to get pairs with correlation above the threshold\n",
        "    # Exclude self-correlations by creating a mask for the upper triangle only\n",
        "    mask = (corr_matrix > threshold) & (np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "    # Find the column pairs with high correlation\n",
        "    high_corr_pairs = []\n",
        "    for col in mask.columns:\n",
        "        for row in mask.index:\n",
        "            if mask.at[row, col]:\n",
        "                high_corr_pairs.append((row, col, corr_matrix.at[row, col]))\n",
        "\n",
        "    # Convert the list to a DataFrame for easier readability\n",
        "    high_corr_df = pd.DataFrame(high_corr_pairs, columns=['Column1', 'Column2', 'Correlation'])\n",
        "\n",
        "    return high_corr_df\n"
      ],
      "metadata": {
        "id": "9P88cF7GdDBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def balance_target_column(df, target_col=\"TARGET_mean\", strategy=\"combine\", random_state=42):\n",
        "\n",
        "    # Split the features and target\n",
        "    X = df.drop(columns=[target_col])\n",
        "    y = df[target_col]\n",
        "\n",
        "    # Choose the resampling strategy\n",
        "    if strategy == \"undersample\":\n",
        "        rus = RandomUnderSampler(random_state=random_state)\n",
        "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
        "    elif strategy == \"oversample\":\n",
        "        smote = SMOTE(random_state=random_state)\n",
        "        X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    else:  # combine undersampling and oversampling\n",
        "        smote_enn = SMOTEENN(random_state=random_state)\n",
        "        X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
        "\n",
        "    # Create a balanced DataFrame with the same feature columns and the resampled target column\n",
        "    df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "    df_resampled[target_col] = y_resampled\n",
        "\n",
        "    return df_resampled"
      ],
      "metadata": {
        "id": "Eeg5AADjC90E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_categorical(df):\n",
        "    df_encoded = df.copy()\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    for col in df_encoded.columns:\n",
        "        if df_encoded[col].dtype == 'object':\n",
        "            df_encoded[col] = label_encoder.fit_transform(df_encoded[col])\n",
        "\n",
        "    return df_encoded"
      ],
      "metadata": {
        "id": "mxiHOgf9dC8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_column_with_small_value(df, col, min_occurrence=0.01):\n",
        "\n",
        "    if df[col].nunique() != 2:\n",
        "        raise ValueError(f\"The column '{col}' does not have exactly 2 unique values.\")\n",
        "\n",
        "    # Calculate value counts and their percentages\n",
        "    value_counts = df[col].value_counts()\n",
        "    total_count = len(df)\n",
        "    value_percentages = value_counts / total_count * 100\n",
        "\n",
        "    # Identify the less frequent value\n",
        "    less_frequent_value = value_percentages.idxmin()\n",
        "    less_frequent_percentage = value_percentages.min()\n",
        "\n",
        "    if less_frequent_percentage < min_occurrence:\n",
        "        print(f\"Column '{col}' has a value '{less_frequent_value}' with a percentage of {less_frequent_percentage:.2f}%. Resampling this column.\")\n",
        "\n",
        "        # Define a new value to replace the less frequent one (e.g., the more frequent value)\n",
        "        more_frequent_value = value_percentages.idxmax()\n",
        "\n",
        "        # Replace less frequent values with the more frequent value\n",
        "        df[col] = df[col].replace(less_frequent_value, more_frequent_value)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "rCMRL56NkIDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing_values_by_digit_strategy(df):\n",
        "    filled_df = df.copy()\n",
        "\n",
        "    for col in filled_df.columns:\n",
        "        # Select only numeric columns to process\n",
        "        if pd.api.types.is_numeric_dtype(filled_df[col]):\n",
        "            # Drop missing values to analyze actual values\n",
        "            non_null_values = filled_df[col].dropna()\n",
        "\n",
        "            # Check if the majority of the values in the column are single digits\n",
        "            single_digit_ratio = (non_null_values.abs() < 10).mean()  # Proportion of single-digit values\n",
        "\n",
        "            # Choose strategy based on the digit count\n",
        "            if single_digit_ratio > 0.5:\n",
        "                # Use mode if most values are single digits\n",
        "                mode_value = non_null_values.mode()[0]\n",
        "                filled_df[col].fillna(mode_value, inplace=True)\n",
        "                print(f\"Filled missing values in '{col}' using mode: {mode_value}\")\n",
        "            else:\n",
        "                # Use mean if most values have two or more digits\n",
        "                mean_value = non_null_values.mean()\n",
        "                filled_df[col].fillna(mean_value, inplace=True)\n",
        "                print(f\"Filled missing values in '{col}' using mean: {mean_value}\")\n",
        "\n",
        "    return filled_df"
      ],
      "metadata": {
        "id": "jzpnzBmpJ6aW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing_with_lr(df):\n",
        "    # Create a copy of the DataFrame to avoid modifying the original one\n",
        "    df_filled = df.copy()\n",
        "\n",
        "    # Identify columns with missing values\n",
        "    target_cols = df.columns[df.isnull().any()].tolist()\n",
        "\n",
        "    for col in target_cols:\n",
        "        # Get the rows with missing values in the current column\n",
        "        missing_rows = df_filled[df_filled[col].isnull()]\n",
        "\n",
        "        for idx in missing_rows.index:\n",
        "            # Row data without the target columns\n",
        "            row_with_nan = df_filled.loc[idx].drop(labels=target_cols)\n",
        "            X_missing = row_with_nan.values.reshape(1, -1)\n",
        "\n",
        "            # Prepare the training data (rows without missing values in the current column)\n",
        "            train_data = df_filled.dropna(subset=[col])\n",
        "            X_train = train_data.drop(columns=target_cols)\n",
        "            y_train = train_data[col]\n",
        "\n",
        "            # Ensure there's enough data to train the model\n",
        "            if len(X_train) > 1:\n",
        "                # Initialize and train the linear regression model\n",
        "                model = LinearRegression()\n",
        "                model.fit(X_train, y_train)\n",
        "\n",
        "                # Predict the missing value and fill it in the DataFrame\n",
        "                predicted_value = model.predict(X_missing)\n",
        "                df_filled.loc[idx, col] = predicted_value\n",
        "            else:\n",
        "                # If there's not enough data, we cannot train the model\n",
        "                # You might want to fill with a different strategy (e.g., mean, median)\n",
        "                df_filled.loc[idx, col] = df_filled[col].mean()  # Or any other strategy\n",
        "\n",
        "    return df_filled"
      ],
      "metadata": {
        "id": "BxybcoE6J3jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_uncommon_cols(df1, df2):\n",
        "    # Find common columns in both DataFrames\n",
        "    common_cols = list(set(df1.columns).intersection(set(df2.columns)))\n",
        "\n",
        "    # Identify dropped columns\n",
        "    dropped_cols_df1 = [col for col in df1.columns if col not in common_cols]\n",
        "    dropped_cols_df2 = [col for col in df2.columns if col not in common_cols]\n",
        "\n",
        "    # Retain only the common columns in both DataFrames\n",
        "    df1 = df1[common_cols]\n",
        "    df2 = df2[common_cols]\n",
        "\n",
        "    return df1, df2, dropped_cols_df1, dropped_cols_df2\n",
        "\n",
        "# Function call\n",
        "train, test, dropped_train, dropped_test = drop_uncommon_cols(train, test)\n",
        "\n",
        "# Print dropped columns\n",
        "print(\"Dropped columns from train DataFrame:\", dropped_train)\n",
        "print(\"Dropped columns from test DataFrame:\", dropped_test)"
      ],
      "metadata": {
        "id": "0nNRP7E6tpp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing step of aggregated and merged dataframe\n",
        "\n",
        "def clean_dataframe(df, threshold=40):\n",
        "    # Drop columns with missing values above a threshold\n",
        "    df = drop_cols(df, threshold)\n",
        "\n",
        "    # Encode categorical columns\n",
        "    df = encode_categorical(df)\n",
        "\n",
        "    # Fill missing values using a strategy (digit-based)\n",
        "    df = fill_missing_values_by_digit_strategy(df)\n",
        "\n",
        "    # Remove duplicate columns\n",
        "    df = remove_duplicate_columns(df)\n",
        "\n",
        "    # Identify columns for logarithmic transformation (non-normal distributions)\n",
        "    check = identify_log_transform_columns(df)\n",
        "\n",
        "    if check:\n",
        "        user_input = input(\"Your dataframe contains columns with non-normal distribution. Would you like to logarithmically transform these columns (yes/no)? \").strip().lower()\n",
        "\n",
        "        # Ensure valid input and proceed accordingly\n",
        "        if user_input == \"yes\":\n",
        "            df = log_transform_columns(df, check)\n",
        "        elif user_input == \"no\":\n",
        "            print(\"No transformations applied.\")\n",
        "        else:\n",
        "            print(\"Invalid input. No transformations applied.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0ebM4IystUbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure test and train dataframes are equal\n",
        "\n",
        "def equalize_dataframes(df1, df2, y=\"Target_column\"):\n",
        "    if list(df1.columns) != list(df2.columns):  # Compare column names\n",
        "        user_input = input(\"Columns are not equal, would you like to equalize? (yes/no): \").strip().lower()\n",
        "\n",
        "        if user_input == \"yes\":\n",
        "            # Find common columns in both DataFrames\n",
        "            common_cols = list(set(df1.columns).intersection(set(df2.columns)))\n",
        "\n",
        "            # Identify dropped columns\n",
        "            dropped_cols_df1 = [col for col in df1.columns if col not in common_cols]\n",
        "            dropped_cols_df2 = [col for col in df2.columns if col not in common_cols]\n",
        "\n",
        "            # Retain only the common columns in both DataFrames\n",
        "            df1 = df1[common_cols]\n",
        "            df2 = df2[common_cols]\n",
        "\n",
        "            print(f\"Removed columns from df1: {dropped_cols_df1}\")\n",
        "            print(f\"Removed columns from df2: {dropped_cols_df2}\")\n",
        "        elif user_input == \"no\":\n",
        "            print(\"No changes made.\")\n",
        "        else:\n",
        "            print(\"Invalid input. No changes made.\")\n",
        "\n",
        "    return df1, df2, y\n"
      ],
      "metadata": {
        "id": "MI9tXFEuYMkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2UltaVDDi-si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r1V8U0Xwi-l7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}